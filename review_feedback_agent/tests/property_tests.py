from dataclasses import dataclass
from typing import Dict, Callable
import functools
from review_feedback_agent.apis import LLM

@dataclass
class Property:
    name: str
    verifier_instruction: str
    expected_output: str

class PropertyRegistry:
    _instance = None
    properties: Dict[str, Property] = {}

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(PropertyRegistry, cls).__new__(cls)
        return cls._instance

    @classmethod
    def register_property(cls, property: Property):
        cls.properties[property.name] = property

    @classmethod
    def get_property(cls, name: str) -> Property:
        return cls.properties.get(name)


def test_property(name: str):
    def decorator(func: Callable[[], dict]):
        @functools.wraps(func)
        def wrapper():
            property_dict = func()
            property = Property(name=name, **property_dict)
            PropertyRegistry.register_property(property)
            return property

        return wrapper()

    return decorator

class PropertyTester:

    def extract_output(self, text: str) -> str:
        start_tag = "<OUTPUT>"
        end_tag = "</OUTPUT>"
        start = text.find(start_tag) + len(start_tag)
        end = text.find(end_tag)
        return text[start:end].strip()

    def test_property(self, property_name: str, feedback: str, review_text: str) -> bool:
        """
        Tests a given property by verifying the feedback output against the expected output.
        
        Params:
            property (str): The name of the property to be tested.
            feedback (str): The feedback generated by the review feedback agent that needs to be verified.
            review_text (str): The original review text to be used in the verification process.
       
        Returns:
            bool: True if the feedback passes the property test, False otherwise.

        Raises:
            ValueError: If the property is not found in the registry.
        """

        verifier_llm = LLM("sonnet-3.5")
        property = PropertyRegistry.get_property(property_name)
        
        if property:
            verification_prompt = f"{property.verifier_instruction} \n\n Verify the output, think step by step, and respond with TRUE or FALSE between <OUTPUT> tags, such as <OUTPUT> TRUE </OUTPUT> or <OUTPUT> FALSE </OUTPUT>."
            verification_result = verifier_llm(f"Original review: {review_text} \n\n Feedback generated by the agent: {feedback}", verification_prompt)

            result = self.extract_output(verification_result)
            
            return result
        else:
            raise ValueError(f"Property '{property_name}' not found in the registry.")


praise_verifier_instruction = """You are given feedback to a review and potentially the review itself. Your task is to identify problematic feedback that lacks actionable content. In particular, our goal is to only have feedback such that upon reading the feedback, the reviewer would want to change their comments.
Reviewer comments are marked with **Reviewer Comment:** and feedback to the reviewer is marked with **Feedback to the reviewer:**.
We are specifically looking for Feedback to the reviewer items that fail to provide actionable guidance. A feedback item is considered non-actionable if it ONLY does one or more of the following without any additional constructive content:

Praises the reviewer's comment
Agrees with the reviewer's point
Only affirms the reviewer's observation
Restates or summarizes the reviewer's comment
Makes a general statement about the importance of the topic

None of these would lead to the reviewer changing their comment.

Actionable feedback MUST include at least one of the following:

Specific suggestions for improving the review comment
Guidance on how to make the comment more constructive or helpful for the authors

Upon reading the feedback, the reviewer should find something to change in their review.

Is there a feedback item starting with '**Feedback to the reviewer:**' that fails to provide actionable feedback as defined above? If so, return True.

Example:

**Reviewer Comment:** "in heterophilic networks, vertices with high structural and semantic similarities are generally farther away from each other" Any evidence for this claim?
**Feedback to the reviewer:** This is a good question that challenges a key assumption of the paper. The feedback appropriately suggests asking for evidence or citations to support this claim.

Example:

**Reviewer Comment:** Researchers already find that heterophily is not always harmful and homophily assumption is not always necessary for GNNs [2,3,4,5]. How does this paper align with these works?
**Feedback to the reviewer:** This is an excellent point that highlights recent developments in the field. The feedback correctly suggests asking the authors to discuss how their work relates to and differs from these existing findings.

Example:

**Reviewer Comment:** How do you select spectral nodes and why? Why do you want to connect the spectral nodes?
**Feedback to the reviewer:** These are important technical questions about the proposed method. The feedback appropriately suggests asking for more details and justification for these design choices.

For such cases, you should return True.

Analyze each Feedback to the reviewer item individually.
Determine if the feedback item contains any part such that upon reading the feedback, the reviewer would want to change their comments.
If a feedback item contains only praise, agreement, restatement, or general statements without any actionable content that would actually help the reviewer change their comment, consider it non-actionable.

First, think step by step and send your reasoning between <REASONING> {{your reasoning}} </REASONING> tags. Then, respond with TRUE or FALSE between <OUTPUT> tags, such as <OUTPUT> TRUE </OUTPUT> or <OUTPUT> FALSE </OUTPUT>.
Return True if there exists at least one non-actionable feedback item as defined above. Return False only if all feedback items include actionable content."""

addressed_to_author_verifier_instruction = """
Task: Identify feedback that incorrectly addresses the paper authors instead of the reviewer.

Input:
- A set of reviewer comments and corresponding feedback items
- Reviewer comments are marked with **Reviewer comment:**
- Feedback is marked with **Feedback to the reviewer:**

Instructions:
1. Analyze each piece of feedback marked with "**Feedback to the reviewer:**".
2. Identify any feedback that appears to be addressing the paper authors rather than the reviewer.
3. Look for key indicators that the feedback is meant for authors, such as:
   - References to "your method," or "your approach"
   - Mentions of "readers" or how to improve the paper for the audience
   - Advice on how to modify experiments, results, or conclusions
   - Any language that assumes the recipient can make direct changes to the paper

Output:
1. Provide your reasoning within <REASONING> tags.
2. Return your final decision within <OUTPUT> tags:
   - TRUE if any feedback item incorrectly addresses the authors
   - FALSE if all feedback items correctly address the reviewer

Examples of incorrect feedback (addressing authors) that should return True:
Example 1:
    **Reviewer Comment:** The authors argued that "previous works mostly focus on addressing specific types of distribution shifts", which seems inproper and incorrect. E.g., the typical works for graph OOD learning EERM [1] and DIR [2] do not assume the type of distribution shifts in their problem formulation."
    **Feedback to the reviewer:** It would be helpful to clarify the distinction between the approach in this paper and previous works like EERM and DIR. Specifically, how does modeling distribution shifts as a mixture of components differ from not assuming specific types of shifts? This would help readers better understand the novelty and contribution.
Example 2:
    **Reviewer Comment:** The main critique of the paper is that it is quite elaborate, and there is no comparison to a far simpler available baseline: send the answer to GPT-J and accept sufficiently high probability answers (this is a simplified version of their L = 2 setup, but requiring training only a single scalar for the classification case).
    **Feedback to the reviewer:** Your suggestion for a simpler baseline comparison is valuable. It would be helpful to include this comparison in the paper, especially for APIs that provide log probabilities. This could demonstrate the advantages of your more complex approach while acknowledging simpler alternatives. Consider discussing why your method might be preferable even when log probabilities are available.
Example 3:
    **Reviewer Comment:** The 5% quality improvement on some datasets seems likely to be a real ensembling effect, but it should be presented as such (that it likely has the same cause as ensembling methods).
    **Feedback to the reviewer:** To strengthen your paper, consider discussing the relationship between FrugalGPT and traditional ensembling techniques. Highlight both similarities and differences, and explain how this relates to the observed quality improvements. This would provide more context for your results and situate your work within the broader field of machine learning.

Example of correct feedback (addressing the reviewer) that should return False:
**Reviewer comment:** The proposed solution could benefit from more details. Exploring the soft-prompt size (i.e., the variable N) would be nice. The N used is not specified in the main text either, which I believe is an important detail.
**Feedback to the reviewer:** To make this feedback more actionable, you could ask the authors to include the value of N used in their experiments in the main text, and suggest adding an ablation study exploring different soft-prompt sizes and their impact on performance and robustness.

Note: The key is to identify feedback that directly instructs or suggests changes to the paper, rather than feedback that helps the reviewer improve their review or reviewing skills.
"""

restate_verifier_instruction = """ 
You are given feedback to a review and potentially the review itself. Analyze each '- **Reviewer Comment**:' '- **Feedback to the reviewer**:' pair. 
Do any of the feedback items simply restate what the comment says without providing any new meaningful and unique suggestions?

Example of feedback that restates the comment without adding value:
* **Reviewer comment:** Can examples or further clarification be given for the 3.1 sentence "enhancing the accountability of the output"? This isn't clear, at least to me.
* **Feedback to the reviewer:** This is a good point that could lead to improved clarity in the paper. To make your comment more actionable, you could ask the authors to provide examples or further clarification for the sentence "enhancing the accountability of the output".

In this example, the feedback essentially repeats the reviewer's request for examples and clarification without providing any new insights or meaningful and unique suggestions on how to improve the comment.

Output:
1. Provide your reasoning within <REASONING> tags.
2. Return your final decision within <OUTPUT> tags:
   - TRUE if you find any feedback items that restate what the comment says
   - FALSE if no feedback items simply restate the comment
"""

comments_in_review_verifier_instruction = """ 
You are given feedback to a review and potentially the review itself. Do all the items starting with '- **Reviewer Comment**:' appear verbatim in the original review?
"""

@test_property("praise_feedback")
def praise_feedback_test():
    return {
        "verifier_instruction": praise_verifier_instruction,
        "expected_output": "false",  # means that there are no comments that only express praise
    }

@test_property("addressed_to_author")
def addressed_to_author_test():
    return {
        "verifier_instruction": addressed_to_author_verifier_instruction,
        "expected_output": "false",  # means that there are no comments that are addressed to the author
    }

@test_property("restate_reviewer")
def addressed_to_author_test():
    return {
        "verifier_instruction": restate_verifier_instruction,
        "expected_output": "false",  # means that there are no comments that restate the reviewer
    }

@test_property("comments_in_review")
def addressed_to_author_test():
    return {
        "verifier_instruction": comments_in_review_verifier_instruction,
        "expected_output": "true",  # means that all reviewer comments in feedback output are in the review
    }
